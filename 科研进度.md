# 科研进度



## 11-06

### 学习bert模型的代码

先把代码跑起来

处理输入数据：

>  squad的场景入手

### Estimator 使用

这个bert的应用代码使用的是基于estimator的开发框架，感觉使用起来也挺方便的，思路上大概就是提供一个模型的构建函数，提供一个输入数据的提供函数，然后就可以调用estimator类的train和eval方法对模型进行训练和验证。

```python
def model_fn_builder(bert_config, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings):
  """Returns `model_fn` closure for TPUEstimator."""

  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """The `model_fn` for TPUEstimator."""
	...
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)
    else:
      raise ValueError(
          "Only TRAIN and PREDICT modes are supported: %s" % (mode))
    return output_spec

  return model_fn
```

使用一种类似工厂模式的方式，只不过产出的不是一个类而是一个函数，在模型函数里面涉及到一个产生模型的函数：

```python
def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,
                 use_one_hot_embeddings):
  """Creates a classification model."""
  model = modeling.BertModel(
      config=bert_config,
      is_training=is_training,
      input_ids=input_ids,
      input_mask=input_mask,
      token_type_ids=segment_ids,
      use_one_hot_embeddings=use_one_hot_embeddings)

    ...

  (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])

  return (start_logits, end_logits)		
```

从这个模型可以学习到，将构造模型的部分变成一个个函数，输入输出都是tensor就可以一定程度上增加代码的可移植性。在这个代码中使用的modeling类就是使用了预先定义好的bert模型。

这个函数返回了两个tensor，表示输出的参数，然后再model_fn中设置计算梯度以及训练的op，传给spec，所以model函数的作用就是构造 接受来自输入函数的tensor，并转化成spec输出 的函数。







处理输入数据的模块，这边的处理方法都是转化成tfrecords的形式保存起来，然后读取。

```python
def convert_examples_to_features(examples, tokenizer, max_seq_length,
                                 doc_stride, max_query_length, is_training,
                                 output_fn):
  """Loads a data file into a list of `InputBatch`s."""
...

      feature = InputFeatures(
          unique_id=unique_id,
          example_index=example_index,
          doc_span_index=doc_span_index,
          tokens=tokens,
          token_to_orig_map=token_to_orig_map,
          token_is_max_context=token_is_max_context,
          input_ids=input_ids,
          input_mask=input_mask,
          segment_ids=segment_ids,
          start_position=start_position,
          end_position=end_position,
          is_impossible=example.is_impossible)

      # Run callback
      output_fn(feature)

      unique_id += 1

```

这里比较值得注意的一点就是，写入tfrecord的方法是把写入函数当作参数传给函数，函数在获取数据的时候进行调用。

### BERT的使用

在训练和使用bert的时候需要输入段落，bert本身的任务是根据上下文预测缺失单词，或者根据前文预测后文，输入的类型包括：

1. input_ids
2. input_mask
3. segment_ids

