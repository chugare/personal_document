# 法律文书摘要综述

法律文书中，通过证据表达来得到最事件整体脉络的理解的过程是法官常常需要面对的问题，面对大量的多种类型证据，梳理起来具有一定的难度和复杂性，例如在案件XXX中，法官需要对于多个事件的多个事实的细节进行判断，证人的证词之间往往有相互印证的情况，如果有自动化的生成工具能够根据证词，找出证人之间相互印证的点，将有助于法官对于实际情况进行判断，除此之外，法官在撰写法律文书的时候往往会遵照一定的规则和书写规范，在这个过程中如果借助文本自动化的生成工具，也将极大的提高法官的工作效率。

在这个应用背景下，涉及到自然语言处理领域的多个应用方面，分别是事实信息的抽取，文本的自动生成，或者将这个问题看作一个整体，作为一个文本自动摘要的问题进行研究。现如今的nlp领域对于文本的自动生成和文本自动摘要都有很高的兴趣，文本自动摘要的方法也在不断地创新，效果不断的变好，我要研究的内容，重点关注于使用文本自动摘要技术从法律文书的证据信息中生成事实的摘要，并且使用文本生成技术来改善文本自动生成的效果和质量。

## 一.研究背景

文本摘要技术分为抽取式摘要和生成式摘要两种，抽取式摘要侧重于从原文中找出和文本中心相关联的句子或者单词，将其作为文书的摘要选择出来，主要的过程是对句子进行打分或者分类的过程。而生成式摘要的摘要的产生过程则是由一步步的单词生成完成的，最终由模型判定生成结束或者自行截断。

#### 抽取式文本摘要

抽取式的文本摘要具有抽取的文本可读性高，和文章内容关联强的优点，但是抽取式的文本摘要模型在文章中没有明确总结的时候无法对文章内容进行进一步的抽象。抽取式摘要的一般方法分为打分和分类。

- 打分式

打分就是对原文中的段落按照总结的作用进行打分，具有较高总结特性的文本具有更高的的得分。这一类方法往往是基于直接的计算的，例如textRank，lexRank，有些使用了神经网络的模型也会产生一个评分，作为段落是否可以成为摘要的依据。

- 分类式

分类的方式也在一定程度上基于对原始文本的打分。一般而言，将文本摘要看作是一个分类任务就是要将某一个句子是否属于摘要作为二分类的问题，但是往往没有现成的数据集可以使用，因此，在有的论文中提出了使用文本相似度或者评价指标的方式来为文本设置标签【Neural Document Summarization by Jointly Learning to Score and Select Sentences】。不过在最近也有提出使用生成式模型来得到句子的摘要得分，来提高打标签的效果的方法【Neural Latent Extractive Document Summarization  】

#### 生成式文本摘要

由于s2s的生成式模型在机器翻译领域取得了比较好的成果，所以在不久之后就被应用在生成式文本摘要之中，【A Neural Attention Model for Abstractive Sentence Summarization】使用attention机制对原文进行编码，同时使用简单的语言模型，来生成摘要文本。【Neural machine translation by jointly learning to align and translate.  】是处理机器翻译任务，其中为了保证文本的长效依赖，让每一个rnn的生成步都对前面的所有的步骤进行attention的操作也在后续的研究中被大量借鉴。

后续的方法大部分都是基于这种encoder-decoder的模型的，主要区别体现在特征提取和生成步骤上进行一些优化。有些研究认为摘要过程中，对信息应当进行适当的筛查，这样才可以体现出摘要和机器翻译的区别【Selective Encoding for Abstractive Sentence Summarization】，所以引入了门控机制对输入的信息进行屏蔽。也有使用了强化学习的方法，提高文本生成效果的。【A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization】。而提高生成质量的方法也有一些，Beam search被广泛使用在文本生成类的任务之中。对于他的改进方法也有很多，从实验结果上看都可以一定程度上提高生成文本的可读性和效果，除此之外，还有根据文本的统计信息对训练数据重新平衡的方法，能改善生成文本中生成冗余的现象。



### 二.方法介绍

下面是文本摘要的各个部分的处理方法的一些详细介绍。

#### 1.特征处理

在做特征处理的时候，需要考虑的事情是特征的提取方式以及方向，一般而言，特征抽取的目的是为了从完整的句子中获取向量化的表示。可以实现的方式有很多种，例如词袋模型，tf-idf值，word2vector词嵌入，都可以将单词转化成向量，进而整合一整个句子。这些特征处理方式往往都很简单，处理速度也很快，（除了word2vec需要额外的训练时间之外）。但是这些简单的文本表示方式带来了模型体量的压力，信息不全面的问题。

词袋模型的实现非常简单，设置一个$d$维全0向量$v$，$d$表示词典的大小，如果某一个单词在句子之中出现了，那么这个单词在字典上的编号对应的位置就置1。这样的话模型接受的输入就是一个d维的向量，而且这是每一个词的向量，如果要是表示一个有序的句子，向量的维度会更大。这就限制了模型的层数，同时这种单纯的1和0也无法反映出一些其他的信息。

word2vector是目前依然很流行的词嵌入方式，在很多nlp问题上都作为词嵌入的首选方案，word2vector可以产生固定大小的向量，与训练时候的设置有关，而且这个向量空间具有语义特性，即在语义上相似的单词在向量空间上的映射值之间的余弦距离也是相近的。



上述的若干方法都是在NLP领域中的基础的特征方法，从单词或字的最直接的表示来获取向量表示，这些方法的有点事简单，但是缺点就是表示信息的片面和单一，为了解决这个问题，在文本摘要的问题上往往需要对输入的信息进行一些后续处理，提纯和压缩信息。

1. skip-thought

word2vector方法是对词的向量表示进行学习，对于不定长度的句子也有一些方法可以表示，比较简单的方式是skip-thought，将句子通过RNN编码后，再使用类似于word2vector的前后预测方式来训练出编码模型，其优点是可以将句子映射成固定长度的向量，并且映射的向量空间具有语义特性。

2. 主题特征

使用主题特征【A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization】

很多相关的论文都提到了类似的方法，使用LDA的语言模型来给出原文中单词的主题信息，再将主题信息补充到单词的embedding向量中

3. 门控机制

encoder-decoder模型最来自于机器翻译的任务，机器翻译的过程中，生成文本的长度应该和输入文本的长度基本上保持一致，传递的信息应当也是完整的保留的，在处理文本摘要任务的问题的时候，就需要在这个生成模型的基础上对原始的输入信息进行筛选。在这篇研究中，使用的方法是训练一个门控选择器，当输入的信息被认为是合适输入的，就将门口的值置1，否则置为零，让这个信息无法传播到decoder的步骤

![1573201399583](C:\Users\CHUGA\AppData\Roaming\Typora\typora-user-images\1573201399583.png)

4. 事实信息

很多文本摘要的模型都只是关注于文本之间的简单关系，这会导致有时候生成的文本语义上存在缺失和矛盾的情况，下面这篇研究就是提出了在encoder的时候将原始文本中的事实关系以类似于三元组的形式表示，并编码。

这里使用的是一个英文的事实关系信息编码工具OpenIE，这个工具本身是开源的，但是只能对英文起作用。

![1573203304151](C:\Users\CHUGA\AppData\Roaming\Typora\typora-user-images\1573203304151.png)

【Faithful to the Original: Fact-Aware Neural Abstractive Summarization】

经过事实信息的处理，可以将原始的信息转化成如下的二元组形式(taiwan; price) (share; price) (lower; tuesday).

![1573203518137](C:\Users\CHUGA\AppData\Roaming\Typora\typora-user-images\1573203518137.png)

将这个关系转化成输入的向量，然后和句子本身的encoder结果，共同做attention，作为最终解码器的输入。

综上所述，在文本摘要的问题中，特征处理的部分要做的就是尽可能涵盖需要总结的特征内容，这些特征可能来自句子本身的表达方式，词语前后的组合关系。也可以通过一些额外的机制来补充信息向量，来提高生成模型产出内容的效果。

#### 2.生成模型

特征处理关注的主要是encoder的部分，而生成模型所关注的更多的是decoder的部分，

文本的生成模块的功能是提高生成文本的阅读效果，同时结合文本本身的信息。包含模型的设计，优化策略两个问题。

模型选择和优化策略的区别在于，前者表示的是模型在训练以及生成中进行的操作，或者是增加可训练的模块，后者则是模型在生成中采用的一些提升的手段。

**+模型选择**

seq2seq是生成模型的最简单的选择，这个设计本身就会包含很多变体，使用LSTM亦或是GRU，或者是使用CNN作为生产模型，都是可以作为对比模型的

- LSTM【】



- GRU【Selective Encoding for Abstractive Sentence Summarization】



- CNNs2s【A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization】



- attention机制

在上述模型的基础之上添加attention机制是常见的思路。【A Neural Attention Model for Abstractive Sentence Summarization】

**+ 修改损失函数**

- 使用互信息作为损失函数的一部分，同时修改互信息的计算方式，使得互信息【Mutual information and diverse decoding improve neural machine translation】

#### 3.优化策略

目前可以提高文本生成质量的策略有很多，对于评论文本生成质量的提高，有一些前置实验，一般而言生成文本的质量主要面临的问题是文本的重复性和文本的平凡语义的问题，这两种问题出现的主要的原因都是s2s模型在使用大数据集进行训练的时候，数据集中的不均衡性导致的，所以一些主流的方法都是借助减少训练文档中的重复单词，或者是为训练文档中的语料按照文本本身的特异性进行打分，借此重新平衡训练集的权重。

主要的解决方案有以下几种。

- BEAMsearch机制（以及其优化策略 ）

【Mutual information and diverse decoding improve neural machine translation】使用改变互信息的计算方法的方式，改变beam search过程中的优化方向

【A Simple, Fast Diverse Decoding Algorithm for Neural Generation-2016] 】使用多元化beam search方法，

- 训练权重的re-weight机制 【Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method】

#### 4.强化学习方法

在很多17-18年的论文中都涉及到了强化学习的方法，强化学习的方式在文本摘要当中会出现，在对话系统中也会出现，一般强化学习的模式为生成式对抗或者基于一些常见指标的强化学习。

- 使用基本的强化学习算法SCST【A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization】
- 使用对抗学习的方法【Adversarial learning for neural dialogue generation】
- 同样是提高生成效果的方法【Deep reinforcement learning for dialogue generation】



以上方法大部分式在生成式文本摘要的问题中进行的

#### 5.抽取式摘要方法

很多的论文中出现了抽取式摘要和生成式摘要结合的办法，比如point网络，基于GMM的学习方式等。



【 Jointly Learning Topics in Sentence Embedding for Document Summarization 】中使用了一种基于无监督学习的主题模型方法，对文本的句子编码应用聚类分析，产生每一个句子的主题向量，再根据主题向量来预测某一个句子是否是属于摘要句子的。





### 三.数据集以及模块

BERT

在将句子转化成向量表示的问题上，BERT是一个目前比较流行的方式。

BERT是使用双向的transformer来进行文本缺词填空的预训练模型，在完成预训练之后，bert可以实现将固定大小的文本输入并进行编码。

这也是当前NLP各个任务提高产出效果的主流方法，通过这种预训练的方式能够很好的帮助下游任务的完成，结合这种方式来提高文本生成的效果也是目前可以采用的方法之一。

DUC

CNN/Daily Mail【Teaching Machines to Read and Comprehend】





### 四.验证方式

ROUGE

文本翻译和文本摘要的常用验证方式，分为ROUGE-1，ROUGE-2和ROUGE-l

BLEU



